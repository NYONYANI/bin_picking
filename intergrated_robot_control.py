# intergrated_robot_control_with_fk_validation.py (ìˆœê¸°êµ¬í•™ ê²€ì¦ ê¸°ëŠ¥ ì¶”ê°€)

import os
import time
import math
import numpy as np
import threading
import ctypes
import cv2
import pyrealsense2 as rs
import open3d as o3d

# PyTorch, YOLO, SAM ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
import torch
from segment_anything import sam_model_registry, SamPredictor
from ultralytics import YOLO
from sklearn.decomposition import PCA

# Scipy ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from scipy.spatial.transform import Rotation as ScipyRotation

# --- [ì¶”ê°€] ìˆœê¸°êµ¬í•™(Forward Kinematics) ê³„ì‚° í´ë˜ìŠ¤ ---
class ForwardKinematics:
    """
    Doosan A0509 ëª¨ë¸ì˜ DH íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆœê¸°êµ¬í•™ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    def __init__(self):
        # PDFì˜ A0509 ì‚¬ì–‘(mm)ì„ ë¯¸í„°(m) ë‹¨ìœ„ë¡œ ì •ì˜ 
        # d1=155.5, d2=409, a1=367, d3=0, d4=124
        d1 = 0.1555
        a1 = 0.367
        d2 = 0.409 # PDFì˜ d2ëŠ” ë‘ ë²ˆì§¸ ë§í¬ì˜ ê¸¸ì´(a2)ì— í•´ë‹¹
        d3 = 0.0
        d4 = 0.124

        # Doosan Robot A0509ì˜ í‘œì¤€ DH íŒŒë¼ë¯¸í„° (Standard DH)
        # alpha, a, d, theta ìˆœì„œ
        self.dh_params = [
            # alpha(i-1), a(i-1),    d(i),      theta(i) - joint variable
            [0,             0,          d1,         0],
            [np.pi/2,       a1,         0,          0],
            [0,             d2,         0,          np.pi/2], # d2ê°€ a2 ì—­í• 
            [np.pi/2,       0,          d3 + d4,    0], # d3, d4ëŠ” Zì¶• offset
            [-np.pi/2,      0,          0,          0],
            [np.pi/2,       0,          0,          0]
        ]

    def _create_dh_matrix(self, alpha, a, d, theta):
        """ ë‹¨ì¼ DH ë³€í™˜ í–‰ë ¬ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ """
        cos_theta = np.cos(theta)
        sin_theta = np.sin(theta)
        cos_alpha = np.cos(alpha)
        sin_alpha = np.sin(alpha)
        
        T = np.array([
            [cos_theta, -sin_theta * cos_alpha,  sin_theta * sin_alpha, a * cos_theta],
            [sin_theta,  cos_theta * cos_alpha, -cos_theta * sin_alpha, a * sin_theta],
            [0,          sin_alpha,             cos_alpha,            d],
            [0,          0,                     0,                    1]
        ])
        return T

    def compute_fk(self, joint_angles_deg):
        """
        ì£¼ì–´ì§„ 6ì¶• ê´€ì ˆ ê°ë„(degree)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë³€í™˜ í–‰ë ¬ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        """
        if len(joint_angles_deg) != 6:
            raise ValueError("ê´€ì ˆ ê°ë„ëŠ” 6ê°œì—¬ì•¼ í•©ë‹ˆë‹¤.")
            
        joint_angles_rad = np.deg2rad(joint_angles_deg)
        
        T_final = np.identity(4)
        for i in range(6):
            alpha, a, d, theta_offset = self.dh_params[i]
            # Joint variable ì¶”ê°€ (A0509 ëª¨ë¸ì˜ íŠ¹ì • theta offset ì ìš©)
            theta = joint_angles_rad[i]
            if i == 2: # Joint 3
                theta += theta_offset
            
            T_i = self._create_dh_matrix(alpha, a, d, theta)
            T_final = T_final @ T_i
            
        return T_final
    
    def get_pose_from_matrix(self, T):
        """
        4x4 ë³€í™˜ í–‰ë ¬ì—ì„œ ìœ„ì¹˜(mm)ì™€ ì˜¤ì¼ëŸ¬ ê°(XYZ, degree)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        position = T[:3, 3] * 1000.0  # ë¯¸í„°ë¥¼ ë°€ë¦¬ë¯¸í„°ë¡œ ë³€í™˜
        
        rotation_matrix = T[:3, :3]
        r = ScipyRotation.from_matrix(rotation_matrix)
        # ë¡œë´‡ ì œì–´ê¸°ì™€ ë™ì¼í•œ 'xyz' ìˆœì„œì˜ ì˜¤ì¼ëŸ¬ ê° ì‚¬ìš©
        euler_angles = r.as_euler('xyz', degrees=True)
        
        return list(position) + list(euler_angles)

# ----------------------------------------------------------------

# --- 1. ë¹„ì „ ì²˜ë¦¬ í´ë˜ìŠ¤ ---
class PickingProcessor:
    def __init__(self, sam_checkpoint_name: str = "sam_vit_h.pth", yolo_model_name: str = "yolov11m.pt"):
        self.sam_checkpoint_name = sam_checkpoint_name
        self.yolo_model_name = yolo_model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.predictor_sam = None
        self.model_yolo = None
        self._load_models()

    def _load_models(self):
        try:
            try:
                script_dir = os.path.dirname(os.path.abspath(__file__))
            except NameError:
                script_dir = os.getcwd()

            sam_model_path = os.path.join(script_dir, self.sam_checkpoint_name)
            if not os.path.exists(sam_model_path):
                raise FileNotFoundError(f"ì˜¤ë¥˜: SAM ëª¨ë¸ '{sam_model_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            sam = sam_model_registry["vit_h"](checkpoint=sam_model_path)
            sam.to(self.device)
            self.predictor_sam = SamPredictor(sam)
            self.model_yolo = YOLO(self.yolo_model_name)
            print("âœ… AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            print(f"âŒ AI ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            exit(1)

    def detect_and_segment(self, rgb_image: np.ndarray):
        self.predictor_sam.set_image(rgb_image)
        results = self.model_yolo(rgb_image)
        boxes = results[0].boxes.xyxy.cpu().numpy()
        
        detections = []
        if boxes.size == 0:
            return detections

        for box in boxes:
            masks, scores, _ = self.predictor_sam.predict(box=np.array(box), multimask_output=True)
            mask = masks[np.argmax(scores)]
            detections.append({'box': box, 'mask': mask})
            
        return detections

    def analyze_point_cloud(self, points: np.ndarray):
        if points is None or len(points) < 10:
            return None
        
        try:
            points = points[np.any(points != 0, axis=1)]
            if len(points) < 10:
                print("  âš ï¸ PCA ë¶„ì„ì„ ìœ„í•œ ìœ íš¨ í¬ì¸íŠ¸ ë¶€ì¡±")
                return None
                
            pca = PCA(n_components=3)
            pca.fit(points)
            center = pca.mean_
            long_axis_vec = pca.components_[0]
            return {'center_cam': center, 'arrow_direction': long_axis_vec}
        except Exception as e:
            print(f"âŒ PCA ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

# --- 2. ë¡œë´‡ ì œì–´ í´ë˜ìŠ¤ ---
class DoosanRobot:
    GRIPPER_PIN_A = 0 
    GRIPPER_PIN_B = 1

    def __init__(self, dll_path): 
        self.robot_ctrl = None
        try:
            print(f"DLL ë¡œë“œ ì‹œë„: {dll_path}")
            self.dsr = ctypes.WinDLL(dll_path)
            self._define_functions()
            self.robot_ctrl = self.dsr._CreateRobotControl()
            if not self.robot_ctrl:
                raise ConnectionError("ë¡œë´‡ ì œì–´ í•¸ë“¤ ìƒì„± ì‹¤íŒ¨")
            print("âœ… ë¡œë´‡ ì œì–´ í•¸ë“¤ ìƒì„± ì„±ê³µ.")
        except Exception as e:
            print(f"âŒ DRFL DLL ë¡œë“œ ë˜ëŠ” í•¸ë“¤ ìƒì„± ì‹¤íŒ¨: {e}")
            
    def _define_functions(self):
        self.dsr._CreateRobotControl.restype = ctypes.c_void_p
        self.dsr._OpenConnection.argtypes = [ctypes.c_void_p, ctypes.c_char_p, ctypes.c_uint]
        self.dsr._OpenConnection.restype = ctypes.c_bool
        self.dsr._CloseConnection.argtypes = [ctypes.c_void_p]
        self.dsr._DestroyRobotControl.argtypes = [ctypes.c_void_p]
        self.dsr._ManageAccessControl.argtypes = [ctypes.c_void_p, ctypes.c_int]
        self.dsr._ManageAccessControl.restype = ctypes.c_bool
        self.dsr._SetRobotMode.argtypes = [ctypes.c_void_p, ctypes.c_int]
        self.dsr._SetRobotMode.restype = ctypes.c_bool
        self.dsr._SetRobotControl.argtypes = [ctypes.c_void_p, ctypes.c_int]
        self.dsr._SetRobotControl.restype = ctypes.c_bool
        self.dsr._GetRobotState.argtypes = [ctypes.c_void_p]
        self.dsr._GetRobotState.restype = ctypes.c_int
        self.dsr._MoveL.argtypes = [ctypes.c_void_p, ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float), ctypes.c_float, ctypes.c_int, ctypes.c_int, ctypes.c_float, ctypes.c_int]
        self.dsr._MoveL.restype = ctypes.c_bool
        self.dsr._GetCurrentPose.argtypes = [ctypes.c_void_p, ctypes.c_int]
        self.dsr._GetCurrentPose.restype = ctypes.POINTER(ctypes.c_float * 6)
        self.dsr._SetToolDigitalOutput.argtypes = [ctypes.c_void_p, ctypes.c_int, ctypes.c_bool]
        self.dsr._SetToolDigitalOutput.restype = ctypes.c_bool
        
        # --- [ì¶”ê°€] _get_current_posj í•¨ìˆ˜ ì •ì˜ ---
        # ì´ í•¨ìˆ˜ëŠ” í˜„ì¬ ë¡œë´‡ì˜ 6ì¶• ê´€ì ˆ ê°ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. 
        self.dsr._get_current_posj.argtypes = [ctypes.c_void_p]
        self.dsr._get_current_posj.restype = ctypes.POINTER(ctypes.c_float * 6)
        # -------------------------------------------

    def connect(self, ip="192.168.137.100", port=12345):
        if not self.robot_ctrl: return False
        if not self.dsr._OpenConnection(self.robot_ctrl, ip.encode('utf-8'), port):
            print(f"âŒ ë¡œë´‡ ì—°ê²° ì‹¤íŒ¨! (IP: {ip})")
            return False
        print(f"âœ… ë¡œë´‡ ì—°ê²° ì„±ê³µ. (IP: {ip})")
        if not self.dsr._ManageAccessControl(self.robot_ctrl, 0):
            print("âŒ ì ‘ê·¼ ê¶Œí•œ ì„¤ì • ì‹¤íŒ¨!")
            self.disconnect()
            return False
        print("âœ… ì ‘ê·¼ ê¶Œí•œ ì„¤ì • ì„±ê³µ.")
        time.sleep(1)
        if not self.dsr._SetRobotControl(self.robot_ctrl, 3):
            print("âŒ ì„œë³´ ì˜¨ ì‹œë„ ì‹¤íŒ¨.")
            self.disconnect()
            return False
        start_time = time.time()
        while time.time() - start_time < 7.0:
            if self.dsr._GetRobotState(self.robot_ctrl) == 1:
                print("âœ… ì„œë³´ í™œì„±í™” ì™„ë£Œ.")
                if self.dsr._SetRobotMode(self.robot_ctrl, 1):
                    print("âœ… ìë™ ëª¨ë“œ ì„¤ì • ì„±ê³µ.")
                    return True
                else:
                    print("âŒ ìë™ ëª¨ë“œ ì„¤ì • ì‹¤íŒ¨.")
                    self.disconnect()
                    return False
            time.sleep(0.1)
        print("âŒ ì„œë³´ í™œì„±í™” ì‹œê°„ ì´ˆê³¼!")
        self.disconnect()
        return False

    def disconnect(self):
        if self.robot_ctrl:
            self.dsr._CloseConnection(self.robot_ctrl)
            self.dsr._DestroyRobotControl(self.robot_ctrl)
            print("âœ… ë¡œë´‡ ì—°ê²° ì¢…ë£Œ ë° í•¸ë“¤ í•´ì œ ì™„ë£Œ.")
    
    def get_current_pose_tcp(self):
        if not self.robot_ctrl: return None
        pose_ptr = self.dsr._GetCurrentPose(self.robot_ctrl, 1)
        if pose_ptr:
            return list(pose_ptr.contents)
        return None

    # --- [ì¶”ê°€] í˜„ì¬ ê´€ì ˆ ê°ë„(joint)ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë©”ì„œë“œ ---
    def get_current_joint_angles(self):
        if not self.robot_ctrl: return None
        # _get_current_posj í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ê´€ì ˆ ê°ë„ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        joint_ptr = self.dsr._get_current_posj(self.robot_ctrl)
        if joint_ptr:
            return list(joint_ptr.contents)
        print("âŒ í˜„ì¬ ê´€ì ˆ ê°ë„ ì½ê¸° ì‹¤íŒ¨.")
        return None
    # ----------------------------------------------------

    def move_l(self, pos, vel=100.0, acc=100.0, wait=True):
        if not self.robot_ctrl or len(pos) != 6: return False
        pos_ctype = (ctypes.c_float * 6)(*pos)
        vel_ctype = (ctypes.c_float * 2)(vel, vel)
        acc_ctype = (ctypes.c_float * 2)(acc, acc)
        res = self.dsr._MoveL(self.robot_ctrl, pos_ctype, vel_ctype, acc_ctype, 0.0, 0, 0, 0.0, 0)
        
        if res and wait:
            # ì´ë™ ëª…ë ¹ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
            time.sleep(0.2) 
            while self.dsr._GetRobotState(self.robot_ctrl) == 2: # STATE_MOVING
                time.sleep(0.1)
        elif not res:
            print("âŒ MoveL ì‹¤í–‰ ì‹¤íŒ¨.")
        return res

    def move_to_home(self):
        print("\nğŸ¤– ì´ˆê¸° ìœ„ì¹˜(ì¤€ë¹„ ìì„¸)ë¡œ ì´ë™ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        home_pos_l = [487,51,448,8,164,7] 

        print(f"  -> ëª…ë ¹ ì¢Œí‘œ (X,Y,Z,R,P,Y): {home_pos_l}")
        self.move_l(home_pos_l, vel=150, acc=150)
        print("âœ… ì´ˆê¸° ìœ„ì¹˜ ì´ë™ ëª…ë ¹ ì™„ë£Œ.")
        time.sleep(0.5)
        self.open_gripper()

    def open_gripper(self):
        self.dsr._SetToolDigitalOutput(self.robot_ctrl, self.GRIPPER_PIN_A, False)
        self.dsr._SetToolDigitalOutput(self.robot_ctrl, self.GRIPPER_PIN_B, False)
        time.sleep(0.5)
        
        self.dsr._SetToolDigitalOutput(self.robot_ctrl, self.GRIPPER_PIN_A, True)
        time.sleep(0.5)
        self.dsr._SetToolDigitalOutput(self.robot_ctrl, self.GRIPPER_PIN_B, False)
        print("ğŸ‘ ê·¸ë¦¬í¼ ì—´ê¸° ì™„ë£Œ.")

    def close_gripper(self):
        self.dsr._SetToolDigitalOutput(self.robot_ctrl, self.GRIPPER_PIN_A, False)
        self.dsr._SetToolDigitalOutput(self.robot_ctrl, self.GRIPPER_PIN_B, False)
        time.sleep(0.5)
        
        self.dsr._SetToolDigitalOutput(self.robot_ctrl, self.GRIPPER_PIN_A, False)
        time.sleep(0.5)
        self.dsr._SetToolDigitalOutput(self.robot_ctrl, self.GRIPPER_PIN_B, True)
        print("ğŸ¤ ê·¸ë¦¬í¼ ë‹«ê¸° ì™„ë£Œ.")


# --- 3. í†µí•© ì œì–´ê¸° í´ë˜ìŠ¤ (ë©”ì¸ ë¡œì§) ---
class IntegratedController:
    APPROACH_HEIGHT_M = 0.20 
    Z_LOWER_LIMIT_MM = 160

    def __init__(self, robot_dll_path): 
        self.vision = PickingProcessor()
        self.robot = DoosanRobot(robot_dll_path) 
        # --- [ì¶”ê°€] ìˆœê¸°êµ¬í•™ ê³„ì‚°ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ---
        self.fk_calculator = ForwardKinematics()
        # -----------------------------------------
        self.target_base_pose = None
        self.approach_pose = None 
        self.last_object_yaw_deg = None
        self.pc = rs.pointcloud()
        self._initialize_realsense()
        
        self.R_EF_CAM = np.array([[0, 1, 0], [-1, 0, 0], [0, 0, 1]])
        self.t_EF_CAM = np.array([-80, 32.5, 34.5]) / 1000.0
        
        self.T_EF_CAM = np.eye(4)
        self.T_EF_CAM[:3, :3], self.T_EF_CAM[:3, 3] = self.R_EF_CAM, self.t_EF_CAM
        self.gripper_offset_vector_ef = np.array([0, 0, 0.146])

    def _initialize_realsense(self):
        self.pipeline = rs.pipeline()
        config = rs.config()
        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
        self.profile = self.pipeline.start(config)
        self.align = rs.align(rs.stream.color)
        print("âœ… RealSense ì¹´ë©”ë¼ ì´ˆê¸°í™” ì™„ë£Œ.")

    def _euler_to_rotation_matrix(self, roll_deg, pitch_deg, yaw_deg):
        try:
            euler_angles_XYZ = [roll_deg, pitch_deg, yaw_deg]
            r = ScipyRotation.from_euler('xyz', euler_angles_XYZ, degrees=True)
            rotation_matrix = r.as_matrix()
            return rotation_matrix
        except Exception as e:
            print(f"âŒ Scipy('xyz') ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return np.eye(3)

    def _calculate_target_pose(self, color_frame, depth_frame):
        print("\n'c' í‚¤ ì…ë ¥ ê°ì§€! ê°ì²´ íƒì§€ ë° ìµœì¢… ëª©í‘œ ìì„¸ ê³„ì‚° ì‹œì‘...")
        
        depth_intrinsics = depth_frame.profile.as_video_stream_profile().get_intrinsics()
        rgb_image = cv2.cvtColor(np.asanyarray(color_frame.get_data()), cv2.COLOR_BGR2RGB)
        depth_image = np.asanyarray(depth_frame.get_data())
        
        print("  - 1. ê°ì²´ íƒì§€ ë° ì„¸ê·¸ë©˜í…Œì´ì…˜...")
        detections = self.vision.detect_and_segment(rgb_image=rgb_image)
        if not detections: 
            print("  âŒ 1ë‹¨ê³„ ì‹¤íŒ¨: íƒì§€ëœ ê°ì²´ ì—†ìŒ")
            return
        
        print("  - 2. í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„± ë° ë¶„ì„...")
        mask = detections[0]['mask']

        self.pc.map_to(color_frame)
        points_rs = self.pc.calculate(depth_frame)
        
        vertices = np.asanyarray(points_rs.get_vertices()).reshape(depth_frame.get_height(), depth_frame.get_width())
        points_struct = vertices[mask]
        
        points = np.array([list(p) for p in points_struct])
        
        analysis_result = self.vision.analyze_point_cloud(points)
        if not analysis_result: 
            print("  âŒ 2ë‹¨ê³„ ì‹¤íŒ¨: PCA ë¶„ì„ ì‹¤íŒ¨")
            return

        print("  - 3. ì¢Œí‘œê³„ ë³€í™˜ ë° ëª©í‘œ ìœ„ì¹˜/ìì„¸ ê³„ì‚°...")
        current_pose_mm_deg = self.robot.get_current_pose_tcp()
        if not current_pose_mm_deg: 
            print("  âŒ 3ë‹¨ê³„ ì‹¤íŒ¨: ë¡œë´‡ í˜„ì¬ ìœ„ì¹˜ ì½ê¸° ì‹¤íŒ¨")
            return
            
        pos_mm, rot_deg = current_pose_mm_deg[:3], current_pose_mm_deg[3:]
        t_BASE_EF = np.array(pos_mm) / 1000.0
        R_BASE_EF = self._euler_to_rotation_matrix(*rot_deg)
        T_BASE_EF = np.eye(4); T_BASE_EF[:3, :3], T_BASE_EF[:3, 3] = R_BASE_EF, t_BASE_EF
        
        R_BASE_CAM = T_BASE_EF[:3, :3] @ self.T_EF_CAM[:3, :3]
        P_CAM = analysis_result['center_cam']
        P_BASE_gripper_tip_homogeneous = T_BASE_EF @ self.T_EF_CAM @ np.append(P_CAM, 1.0)
        P_BASE_gripper_tip = P_BASE_gripper_tip_homogeneous[:3]
        offset_vector_base = R_BASE_EF @ self.gripper_offset_vector_ef
        target_pos_flange = P_BASE_gripper_tip - offset_vector_base
        target_pos_flange[0] += 0.01; target_pos_flange[1] += 0.05 ;target_pos_flange[2] +=0.0
        
        obj_dir_cam = analysis_result['arrow_direction']
        obj_dir_base = R_BASE_CAM @ obj_dir_cam

        calculated_yaw_rad = math.atan2(obj_dir_base[1], obj_dir_base[0])
        correction_offset_rad = np.deg2rad(-90.0)
        final_yaw_rad = calculated_yaw_rad + correction_offset_rad
        
        final_yaw_deg = np.rad2deg(final_yaw_rad)
        pitch_deg = 180.0
        roll_deg = 0.0
        
        target_rot_deg = [roll_deg, pitch_deg, final_yaw_deg]
        
        self.target_base_pose = list(target_pos_flange * 1000.0) + list(target_rot_deg)
        self.last_object_yaw_deg = final_yaw_deg

        original_z = self.target_base_pose[2]
        if original_z < self.Z_LOWER_LIMIT_MM:
            self.target_base_pose[2] = self.Z_LOWER_LIMIT_MM
            print(f"  âš ï¸  Zì¶• í•˜ê°• í•œê³„ ì ìš©! ê³„ì‚°ëœ Zê°’({original_z:.2f}mm)ì´ í•œê³„({self.Z_LOWER_LIMIT_MM}mm)ë³´ë‹¤ ë‚®ì•„ ì¡°ì •í•©ë‹ˆë‹¤.")

        print("âœ… ìµœì¢… ëª©í‘œ ìì„¸ ê³„ì‚° ì™„ë£Œ.")
        
        print("ğŸ“Š 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì‹œê°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (ì°½ì„ ë‹«ì•„ì•¼ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰)")
        self._visualize_results(rgb_image, depth_image, depth_intrinsics, T_BASE_EF, P_BASE_gripper_tip, analysis_result)

    def _move_robot_to_target(self):
        if self.target_base_pose is None:
            print("âš ï¸ ë¨¼ì € 'c' í‚¤ë¥¼ ëˆŒëŸ¬ ëª©í‘œ ì¢Œí‘œë¥¼ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤.")
            return

        print(f"\n'm' í‚¤ ì…ë ¥ ê°ì§€! ê°ì²´ ìƒë‹¨ ì ‘ê·¼ ìœ„ì¹˜ë¡œ ì´ë™í•©ë‹ˆë‹¤...")
        
        final_pos_mm = np.array(self.target_base_pose[:3])
        final_rot_deg = np.array(self.target_base_pose[3:])
        approach_pos_mm = final_pos_mm + np.array([0, 0, self.APPROACH_HEIGHT_M * 1000.0])
        
        approach_robot_target = list(approach_pos_mm) + list(final_rot_deg)
        
        self.approach_pose = approach_robot_target
        
        print(f"  - ëª©í‘œ ì§€ì  +{self.APPROACH_HEIGHT_M*100:.0f}cm ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        print(f"  - ì ‘ê·¼ ëª©í‘œ: Pos(mm)={np.round(self.approach_pose[:3], 2)}, Rot(deg)={np.round(self.approach_pose[3:], 2)}")
        self.robot.move_l(self.approach_pose, vel=150, acc=150)
        print(f"âœ… ì ‘ê·¼ ìœ„ì¹˜ë¡œ ì´ë™ ì™„ë£Œ.")
        
    def _move_down_and_pick(self):
        if self.target_base_pose is None or self.approach_pose is None:
            print("âš ï¸ ë¨¼ì € 'c'ì™€ 'm'í‚¤ë¥¼ ìˆœì„œëŒ€ë¡œ ëˆŒëŸ¬ ëª©í‘œ ë° ì ‘ê·¼ ì¢Œí‘œë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            return

        print("\n'd' í‚¤ ì…ë ¥ ê°ì§€! íŒŒì§€ ë™ì‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        print("  - 1. ìµœì¢… ëª©í‘œ ìœ„ì¹˜ë¡œ í•˜ê°•...")
        final_target_pose = self.target_base_pose
        print(f"    - ìµœì¢… ëª©í‘œ: Pos(mm)={np.round(final_target_pose[:3], 2)}, Rot(deg)={np.round(final_target_pose[3:], 2)}")
        self.robot.move_l(final_target_pose, vel=80, acc=80)
        print("  âœ… í•˜ê°• ì™„ë£Œ.")
        time.sleep(0.5)

        # --- [ì¶”ê°€] ìˆœê¸°êµ¬í•™ ê¸°ë°˜ ê²€ì¦ ë¡œì§ ---
        self.validate_pose_with_fk()
        # ------------------------------------

        print("  - 2. ê·¸ë¦¬í¼ë¥¼ ë‹«ì•„ íŒŒì§€í•©ë‹ˆë‹¤...")
        self.robot.close_gripper()
        time.sleep(1.0) 

        print("  - 3. ì ‘ê·¼ ìœ„ì¹˜ë¡œ ë³µê·€í•©ë‹ˆë‹¤...")
        print(f"    - ë³µê·€ ëª©í‘œ: Pos(mm)={np.round(self.approach_pose[:3], 2)}, Rot(deg)={np.round(self.approach_pose[3:], 2)}")
        self.robot.move_l(self.approach_pose, vel=150, acc=150)
        print("âœ… íŒŒì§€ ë° ë³µê·€ ë™ì‘ ì™„ë£Œ.")

    # --- [ì¶”ê°€] ìˆœê¸°êµ¬í•™ì„ ì´ìš©í•œ ìì„¸ ê²€ì¦ ë° ì˜¤ì°¨ ê³„ì‚° ë©”ì„œë“œ ---
    def validate_pose_with_fk(self):
        print("\n--- ğŸ¤– ìˆœê¸°êµ¬í•™(FK) ê¸°ë°˜ ì¢Œí‘œ ê²€ì¦ ì‹œì‘ ---")
        
        # 1. í˜„ì¬ ë¡œë´‡ì˜ ì‹¤ì œ ê´€ì ˆ ê°ë„(degree)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        actual_joint_angles = self.robot.get_current_joint_angles()
        if actual_joint_angles is None:
            print("âŒ ê²€ì¦ ì‹¤íŒ¨: í˜„ì¬ ê´€ì ˆ ê°ë„ë¥¼ ì½ì–´ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        # 2. FK ê³„ì‚°ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ì ˆ ê°ë„ë¡œë¶€í„° TCP ì¢Œí‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        fk_matrix = self.fk_calculator.compute_fk(actual_joint_angles)
        fk_pose = self.fk_calculator.get_pose_from_matrix(fk_matrix)
        
        # 3. ë¹„ì „ìœ¼ë¡œ ê³„ì‚°í–ˆë˜ ëª©í‘œ ì¢Œí‘œì™€ ë¹„êµí•©ë‹ˆë‹¤.
        vision_target_pose = self.target_base_pose
        
        # 4. ê²°ê³¼ ì¶œë ¥
        print(f"  - í˜„ì¬ ê´€ì ˆ ê°ë„ (deg): {[f'{q:.2f}' for q in actual_joint_angles]}")
        print("-" * 30)
        print(f"  - ë¹„ì „ ëª©í‘œ ì¢Œí‘œ (X,Y,Z): {[f'{p:.2f}' for p in vision_target_pose[:3]]} (mm)")
        print(f"  - FK ê³„ì‚° ì¢Œí‘œ (X,Y,Z) : {[f'{p:.2f}' for p in fk_pose[:3]]} (mm)")
        print("-" * 30)
        print(f"  - ë¹„ì „ ëª©í‘œ ìì„¸ (R,P,Y): {[f'{r:.2f}' for r in vision_target_pose[3:]]} (deg)")
        print(f"  - FK ê³„ì‚° ìì„¸ (R,P,Y) : {[f'{r:.2f}' for r in fk_pose[3:]]} (deg)")
        print("-" * 30)
        
        # 5. ì˜¤ì°¨ ê³„ì‚°
        pos_error = np.linalg.norm(np.array(vision_target_pose[:3]) - np.array(fk_pose[:3]))
        
        # ìì„¸ ì˜¤ì°¨ëŠ” ê° ì¶•ë³„ë¡œ ë‹¨ìˆœ ì°¨ì´ë¥¼ ê³„ì‚°
        rot_error_r = abs(vision_target_pose[3] - fk_pose[3])
        rot_error_p = abs(vision_target_pose[4] - fk_pose[4])
        rot_error_y = abs(vision_target_pose[5] - fk_pose[5])
        
        print("ğŸ” ì˜¤ì°¨ ë¶„ì„:")
        print(f"  - ìœ„ì¹˜ ì˜¤ì°¨ (Euclidean Distance): {pos_error:.3f} mm")
        print(f"  - ìì„¸ ì˜¤ì°¨ (Roll, Pitch, Yaw): {rot_error_r:.3f}, {rot_error_p:.3f}, {rot_error_y:.3f} (deg)")
        print("--- ê²€ì¦ ì¢…ë£Œ ---\n")
    # -------------------------------------------------------------

    def _get_rotation_matrix_from_vectors(self, vec1, vec2):
        a, b = (vec1 / np.linalg.norm(vec1)).reshape(3), (vec2 / np.linalg.norm(vec2)).reshape(3)
        v = np.cross(a, b)
        c = np.dot(a, b)
        s = np.linalg.norm(v)
        if s < 1e-6: return np.eye(3)
        kmat = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])
        return np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s ** 2))

    def _create_arrow(self, p_start, p_end, shaft_radius=0.001, color=[1, 0, 0]):
        vec = p_end - p_start
        length = np.linalg.norm(vec)
        if length < 1e-6: return None
        arrow = o3d.geometry.TriangleMesh.create_arrow(
            cylinder_radius=shaft_radius, cone_radius=shaft_radius * 2,
            cylinder_height=length * 0.8, cone_height=length * 0.2)
        arrow.paint_uniform_color(color)
        rotation_matrix = self._get_rotation_matrix_from_vectors([0, 0, 1], vec)
        arrow.rotate(rotation_matrix, center=(0, 0, 0))
        arrow.translate(p_start)
        return arrow

    def _visualize_results(self, rgb_image, depth_image, depth_intrinsics, T_BASE_EF, P_BASE_object, target_info):
        T_BASE_CAM = T_BASE_EF @ self.T_EF_CAM
        
        o3d_color = o3d.geometry.Image(cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))
        o3d_depth = o3d.geometry.Image(depth_image)
        rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(
            o3d_color, o3d_depth, depth_scale=1000.0, convert_rgb_to_intensity=False)
        intr = o3d.camera.PinholeCameraIntrinsic(
            depth_intrinsics.width, depth_intrinsics.height, depth_intrinsics.fx, 
            depth_intrinsics.fy, depth_intrinsics.ppx, depth_intrinsics.ppy)
        full_pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd, intr).voxel_down_sample(0.005)
        
        base_frame_geometries = [full_pcd.transform(T_BASE_CAM)]
        base_frame_geometries.append(o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0, 0, 0]))
        
        if target_info and 'arrow_direction' in target_info:
            center, arrow_vec = target_info['center_cam'], target_info['arrow_direction']
            arrow_geom = self._create_arrow(center - arrow_vec * 0.05, center + arrow_vec * 0.05, shaft_radius=0.002, color=[0, 0, 1])
            if arrow_geom: 
                base_frame_geometries.append(arrow_geom.transform(T_BASE_CAM))

        object_marker = o3d.geometry.TriangleMesh.create_sphere(radius=0.01)
        object_marker.paint_uniform_color([1, 0, 0])
        object_marker.translate(P_BASE_object)
        base_frame_geometries.append(object_marker)

        camera_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.05, origin=[0, 0, 0])
        camera_frame.transform(T_BASE_CAM)
        base_frame_geometries.append(camera_frame)

        end_effector_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.05, origin=[0, 0, 0])
        end_effector_frame.transform(T_BASE_EF)
        base_frame_geometries.append(end_effector_frame)

        vis = o3d.visualization.Visualizer()
        vis.create_window(window_name="Robot Base Coordinate System View", width=960, height=540)
        
        for geometry in base_frame_geometries:
            vis.add_geometry(geometry)
            
        vis.run()
        vis.destroy_window()

    def run(self):
        if self.robot.robot_ctrl is None or not self.robot.connect():
            print("ë¡œë´‡ ì—°ê²° ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        time.sleep(1)
        self.robot.move_to_home()
        
        print("\n--- ì¡°ì‘ ì•ˆë‚´ ---\n'c': ê°ì²´ íƒì§€ ë° ëª©í‘œ ìì„¸ ê³„ì‚°\n'm': ê³„ì‚°ëœ ìì„¸ë¡œ +20cm ìœ„ë¡œ ì´ë™ (ì ‘ê·¼)\n'd': í•˜ê°• -> íŒŒì§€ -> ë³µê·€ ë™ì‘ ì‹¤í–‰\n'e': ì´ˆê¸° ìœ„ì¹˜ë¡œ ë³µê·€\n'q': í”„ë¡œê·¸ë¨ ì¢…ë£Œ\n-----------------\n")

        try:
            while True:
                frames = self.pipeline.wait_for_frames()
                aligned_frames = self.align.process(frames)
                color_frame = aligned_frames.get_color_frame()
                depth_frame = aligned_frames.get_depth_frame()
                if not depth_frame or not color_frame: continue

                color_image_bgr = np.asanyarray(color_frame.get_data())

                current_pose = self.robot.get_current_pose_tcp()
                if current_pose:
                    robot_rot_deg = current_pose[3:]
                    robot_text = f"Robot Angle (R,P,Y): {robot_rot_deg[0]:.1f}, {robot_rot_deg[1]:.1f}, {robot_rot_deg[2]:.1f}"
                    cv2.putText(color_image_bgr, robot_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

                if self.last_object_yaw_deg is not None:
                    obj_text = f"Target Yaw: {self.last_object_yaw_deg:.1f}"
                    cv2.putText(color_image_bgr, obj_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                
                cv2.imshow('RealSense Live Feed', color_image_bgr)
                key = cv2.waitKey(1) & 0xFF

                if key == ord('c'): self._calculate_target_pose(color_frame, depth_frame)
                elif key == ord('m'): self._move_robot_to_target()
                elif key == ord('d'): self._move_down_and_pick()
                elif key == ord('e'): self.robot.move_to_home()
                elif key == ord('q'):
                    print("\n'q' í‚¤ ì…ë ¥ ê°ì§€. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
        finally:
            self.pipeline.stop()
            cv2.destroyAllWindows()
            self.robot.disconnect()


if __name__ == "__main__":
    DLL_FOLDER_PATH = r"C:\Users\robot\Desktop\Segmentation\dll"
    
    print("-" * 50)
    print("DLL ë¡œë“œ ì „ ê²½ë¡œ ê²€ì‚¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    try:
        if os.path.exists(DLL_FOLDER_PATH):
             os.add_dll_directory(DLL_FOLDER_PATH)
             print("âœ… DLL ê²€ìƒ‰ ê²½ë¡œ ì¶”ê°€ ì™„ë£Œ.")
        else:
            raise FileNotFoundError
       
        full_dll_path = os.path.join(DLL_FOLDER_PATH, "DRFLWin64.dll")

        print("-" * 50)
        controller = IntegratedController(robot_dll_path=full_dll_path)
        controller.run()
            
    except FileNotFoundError:
        print(f"\n[ì˜¤ë¥˜] '{DLL_FOLDER_PATH}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. âŒ")
    except AttributeError:
        print("\n[ì˜¤ë¥˜] 'os.add_dll_directory' í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   Python 3.8 ì´ìƒ ë²„ì „ì´ í•„ìš”í•©ë‹ˆë‹¤. íŒŒì´ì¬ ë²„ì „ì„ í™•ì¸í•´ì£¼ì„¸ìš”. âŒ")
    except Exception as e:
        print(f"\n[ì˜¤ë¥˜] ê²½ë¡œ ê²€ì‚¬ ë˜ëŠ” ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        
    print("-" * 50)m